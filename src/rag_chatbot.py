import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import Chroma
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_upstage.chat_models import ChatUpstage
from langchain_upstage.embeddings import UpstageEmbeddings
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(override=True)

# --- ìƒìˆ˜ ì •ì˜ ---
PROMPT_TEMPLATES = {
    "Basic": """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. 
        ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚œ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
        {context}
        
        [ì§ˆë¬¸]: 
        {input}
    """,
    "Few-shot": """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. 
        ì•„ë˜ ì˜ˆì‹œì™€ ê°™ì´, ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚œ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        [ì˜ˆì‹œ 1]
        ì§ˆë¬¸: RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?
        ë‹µë³€: RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ê²€ìƒ‰(Retrieval)ì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±(Generation)í•˜ëŠ” ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

        [ì˜ˆì‹œ 2]
        ì§ˆë¬¸: LangChainì˜ ì¥ì ì€?
        ë‹µë³€: LangChainì˜ ì£¼ìš” ì¥ì ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ëª¨ë“ˆí™”í•˜ê³  ê°„ì†Œí™”í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ LLM ëª¨ë¸, ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤, API ë“±ì„ 'ì²´ì¸'ì´ë¼ëŠ” í˜•íƒœë¡œ ì‰½ê²Œ ê²°í•©í•  ìˆ˜ ìˆì–´ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, ê°œë°œ ê³¼ì •ì„ í‘œì¤€í™”í•˜ì—¬ ìƒì‚°ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.
        
        [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
        {context}
        
        [ì§ˆë¬¸]: 
        {input}
    """,
    "Self-Correction": """
        **ì´ˆì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸ (ë‚´ë¶€ìš©)**
        You are an AI assistant. Use the retrieved documents to provide a draft answer to the user's question.
        
        [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
        {context}
        
        [ì§ˆë¬¸]: 
        {input}
        
        **ìˆ˜ì • í”„ë¡¬í”„íŠ¸ (ìµœì¢… ë‹µë³€ìš©)**
        You are an expert AI assistant. Your task is to refine a draft answer based on the provided documents.
        Review the original question, the retrieved documents, and the draft answer.
        Rewrite the draft answer to be more accurate, detailed, and faithful to the documents.
        The final answer must be in Korean.

        [ì›ë³¸ ì§ˆë¬¸]: {input}
        [ê²€ìƒ‰ëœ ë¬¸ì„œ]: {context}
        [ë‹µë³€ ì´ˆì•ˆ]: {draft_answer}

        [ìˆ˜ì •ëœ ìµœì¢… ë‹µë³€]:
    """,
    "CoT": """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.
        ë¨¼ì € ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì£¼ìš” ë‚´ìš©ì„ ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
        ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚œ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
        {context}
        
        [ì§ˆë¬¸]: 
        {input}
        
        [ë‹¨ê³„ë³„ ìƒê°]:
        1. ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
        2. ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ì„ ì°¾ìŠµë‹ˆë‹¤.
        3. ì°¾ì•„ë‚¸ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        
        [ìµœì¢… ë‹µë³€]:
    """
}
DEFAULT_PROMPT_KEY = "Basic"


# --- ëª¨ë¸ ë° ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” (ìºì‹± í™œìš©) ---
@st.cache_resource
def load_components(prompt_key=DEFAULT_PROMPT_KEY):
    """
    LangChainì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸(LLM, ì„ë² ë”©, ë²¡í„°ìŠ¤í† ì–´, ë¦¬íŠ¸ë¦¬ë²„)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    Streamlitì˜ ìºì‹±ì„ í™œìš©í•˜ì—¬ ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    print("[INFO] LangChain ì»´í¬ë„ŒíŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    
    # 1. ChatUpstage LLM ë¡œë“œ
    chat = ChatUpstage()
    
        # 2. Upstage ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embeddings = UpstageEmbeddings(model="embedding-passage")

    # 3. ChromaDB ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings
    )
    
    # 4. ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={'score_threshold': 0.4, 'k': 5}
    )
    
    print("[INFO] ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ.")
    return chat, embeddings, vectorstore, retriever

# --- ì „ì—­ ì»´í¬ë„ŒíŠ¸ (ëŸ°íƒ€ì„ ì´ˆê¸°í™”) ---
chat = None
embeddings = None
vectorstore = None
retriever = None


# --- RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---
def create_rag_chain(history_aware_retriever, prompt_key=DEFAULT_PROMPT_KEY):
    """
    ì„ íƒëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ê¸°ë°˜ìœ¼ë¡œ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    'Self-Correction'ì˜ ê²½ìš° íŠ¹ë³„í•œ 2ë‹¨ê³„ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if prompt_key == "Self-Correction":
        # 1. ì´ˆì•ˆ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸
        draft_prompt_template = """
            You are an AI assistant. Use the retrieved documents to provide a draft answer to the user's question.
            The answer must be in Korean.
            
            [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
            {context}
            
            [ì§ˆë¬¸]: 
            {input}
        """
        draft_prompt = ChatPromptTemplate.from_template(draft_prompt_template)
        draft_chain = create_stuff_documents_chain(chat, draft_prompt)

        # 2. ìˆ˜ì •ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        refine_prompt_template = """
            You are an expert AI assistant. Your task is to refine a draft answer based on the provided documents.
            Review the original question, the retrieved documents, and the draft answer.
            Rewrite the draft answer to be more accurate, detailed, and faithful to the documents.
            The final answer must be in Korean.

            [ì›ë³¸ ì§ˆë¬¸]: {question}
            [ê²€ìƒ‰ëœ ë¬¸ì„œ]: {context}
            [ë‹µë³€ ì´ˆì•ˆ]: {draft_answer}

            [ìˆ˜ì •ëœ ìµœì¢… ë‹µë³€]:
        """
        refine_prompt = ChatPromptTemplate.from_template(refine_prompt_template)
        
        # 3. 2ë‹¨ê³„ ì²´ì¸ ê²°í•©
        # Retrieval ì²´ì¸ -> ì´ˆì•ˆ ìƒì„± -> ìˆ˜ì • ì²´ì¸
        # 'context'ì™€ 'input'ì„ ëê¹Œì§€ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
        refine_chain = (
            {
                "draft_answer": create_retrieval_chain(history_aware_retriever, draft_chain) | itemgetter("answer"),
                "input": itemgetter("input"),
                "context": itemgetter("context"),
            }
            | RunnablePassthrough.assign(question=itemgetter("input")) # 'question' í‚¤ ì¶”ê°€
            | refine_prompt
            | chat
            | StrOutputParser()
        )
        # ìµœì¢… ì¶œë ¥ í˜•ì‹ì„ {"answer": "...", "context": ...} ë¡œ ë§ì¶”ê¸° ìœ„í•œ ë˜í¼
        # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ contextë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
        final_chain = (
             RunnablePassthrough.assign(context=history_aware_retriever)
             | RunnablePassthrough.assign(answer=refine_chain)
             | (lambda x: {"answer": x["answer"], "context": x["context"]})
        )
        return final_chain

    # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
    selected_prompt_template = PROMPT_TEMPLATES.get(prompt_key, PROMPT_TEMPLATES[DEFAULT_PROMPT_KEY])
    
    # 1. QA ì²´ì¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", selected_prompt_template),
        ("human", "{input}"),
    ])

    # 2. ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ê²°í•©í•˜ëŠ” Stuff ì²´ì¸ ìƒì„±
    question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
    
    # 3. ë¦¬íŠ¸ë¦¬ë²„ì™€ QA ì²´ì¸ì„ ê²°í•©í•˜ëŠ” ìµœì¢… RAG ì²´ì¸ ìƒì„±
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain


# --- ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± í•¨ìˆ˜ ---
def get_history_aware_retriever(chat_history):
    """
    ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1. ëŒ€í™” ê¸°ë¡ â†’ ê²€ìƒ‰ì–´ ë³€í™˜ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, "
        "ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. "
        "ì¬êµ¬ì„±ëœ ì§ˆë¬¸ì€ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])

    # 2. ê²€ìƒ‰ì–´ ë³€í™˜ ì²´ì¸ ìƒì„±
    history_aware_retriever_chain = create_history_aware_retriever(
    chat, retriever, contextualize_q_prompt
)

    # 3. ëŒ€í™” ê¸°ë¡ì„ ì²´ì¸ì— ë°”ì¸ë”©
    return RunnablePassthrough.assign(
        chat_history=lambda x: chat_history
    ) | history_aware_retriever_chain


# --- ë©”ì¸ Streamlit ì•± ë¡œì§ ---
def main():
    """
    Streamlit ê¸°ë°˜ì˜ RAG ì±—ë´‡ UIë¥¼ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    st.set_page_config(page_title="RAG Chatbot Demo", page_icon="ğŸ¤–")

    global chat, embeddings, vectorstore, retriever
    if chat is None or embeddings is None or vectorstore is None or retriever is None:
        chat, embeddings, vectorstore, retriever = load_components()

    st.title("ğŸ¤– RAG Chatbot Demo")
    st.caption("IT ê¸°ìˆ  ë¸”ë¡œê·¸ ë° ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.")

    # --- UI ì»¨íŠ¸ë¡¤ëŸ¬ ---
    col1, col2 = st.columns(2)
    with col1:
        rag_enabled = st.toggle("RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±) í™œì„±í™”", value=True)
    with col2:
        prompt_key = st.selectbox(
            "í”„ë¡¬í”„íŒ… ê¸°ë²• ì„ íƒ:",
            options=list(PROMPT_TEMPLATES.keys()),
            index=0,
            disabled=not rag_enabled
        )

    # --- ì„¸ì…˜ ìƒíƒœ ë° ì±„íŒ… ê¸°ë¡ ê´€ë¦¬ ---
    msgs = StreamlitChatMessageHistory(key="chat_history")
    if len(msgs.messages) == 0:
        msgs.add_ai_message("ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” IT ê¸°ìˆ  ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for msg in msgs.messages:
        st.chat_message(msg.type).write(msg.content)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input := st.chat_input(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.chat_message("human").write(user_input)
        msgs.add_user_message(user_input)

        with st.chat_message("ai"):
            # RAG í™œì„±í™” ì‹œ
            if rag_enabled:
                history_aware_retriever = get_history_aware_retriever(msgs.messages)
                rag_chain = create_rag_chain(history_aware_retriever, prompt_key)
                
                response_generator = rag_chain.stream({"input": user_input})
                
                full_response = ""
                response_container = st.empty()
                
                for chunk in response_generator:
                    if "answer" in chunk:
                        full_response += chunk["answer"]
                        response_container.markdown(full_response + "â–Œ")
                response_container.markdown(full_response)
                
                msgs.add_ai_message(full_response)

            # RAG ë¹„í™œì„±í™” ì‹œ (LLM ë‹¨ë… ì‘ë‹µ)
            else:
                llm_chain = (
                    {"input": RunnablePassthrough()}
                    | chat
                    | StrOutputParser()
                )
                response = llm_chain.invoke(user_input)
                st.write(response)
                msgs.add_ai_message(response)
                
    # --- ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥ (ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ) ---
    if 'initial_run' not in st.session_state:
        print(f"\n[INFO] ChromaDB ì»¬ë ‰ì…˜ '{vectorstore._collection.name}'ì— ì €ì¥ëœ ì´ ë¬¸ì„œ ì¡°ê°(Chunk) ìˆ˜: {vectorstore._collection.count()}ê°œ")
        st.session_state['initial_run'] = True


if __name__ == "__main__":
    main()